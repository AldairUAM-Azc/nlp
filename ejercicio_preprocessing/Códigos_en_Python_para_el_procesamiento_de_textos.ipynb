{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w6lsLg77ukw"
   },
   "source": [
    "# Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8UVSvxRmYWw",
    "outputId": "f374f42e-5365-4854-ec63-839b0cdbbb59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,wordpunct_tokenize,sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T88GItVGmg9V",
    "outputId": "6d6bbc3d-f21a-4f3c-8208-c75080484269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n¿Reformar la Ley Federal del Trabajo?', 'es una ocurrencia, que no deja de ser una declaración local.', 'Comentó el presidente, ante la propuesta de la fracción del PRI para que el COVID19 sea considerado riesgo de trabajo.', 'Muchas personas viajaron fuera de Colombia para ponerse la vacuna contra Covid19 #uam y el certificado digital de vacunación el 17-02-2023 no los cubre.']\n",
      "['¿', 'Reformar', 'la', 'Ley', 'Federal', 'del', 'Trabajo', '?', 'es', 'una', 'ocurrencia', ',', 'que', 'no', 'deja', 'de', 'ser', 'una', 'declaración', 'local', '.', 'Comentó', 'el', 'presidente', ',', 'ante', 'la', 'propuesta', 'de', 'la', 'fracción', 'del', 'PRI', 'para', 'que', 'el', 'COVID19', 'sea', 'considerado', 'riesgo', 'de', 'trabajo', '.', 'Muchas', 'personas', 'viajaron', 'fuera', 'de', 'Colombia', 'para', 'ponerse', 'la', 'vacuna', 'contra', 'Covid19', '#', 'uam', 'y', 'el', 'certificado', 'digital', 'de', 'vacunación', 'el', '17', '-', '02', '-', '2023', 'no', 'los', 'cubre', '.']\n",
      "\n",
      "¿Reformar la Ley Federal del Trabajo?\n",
      "['¿', 'Reformar', 'la', 'Ley', 'Federal', 'del', 'Trabajo', '?']\n",
      "es una ocurrencia, que no deja de ser una declaración local.\n",
      "['es', 'una', 'ocurrencia', ',', 'que', 'no', 'deja', 'de', 'ser', 'una', 'declaración', 'local', '.']\n",
      "Comentó el presidente, ante la propuesta de la fracción del PRI para que el COVID19 sea considerado riesgo de trabajo.\n",
      "['Comentó', 'el', 'presidente', ',', 'ante', 'la', 'propuesta', 'de', 'la', 'fracción', 'del', 'PRI', 'para', 'que', 'el', 'COVID19', 'sea', 'considerado', 'riesgo', 'de', 'trabajo', '.']\n",
      "Muchas personas viajaron fuera de Colombia para ponerse la vacuna contra Covid19 #uam y el certificado digital de vacunación el 17-02-2023 no los cubre.\n",
      "['Muchas', 'personas', 'viajaron', 'fuera', 'de', 'Colombia', 'para', 'ponerse', 'la', 'vacuna', 'contra', 'Covid19', '#', 'uam', 'y', 'el', 'certificado', 'digital', 'de', 'vacunación', 'el', '17', '-', '02', '-', '2023', 'no', 'los', 'cubre', '.']\n"
     ]
    }
   ],
   "source": [
    "## Define el texto a utilizar\n",
    "textos = '''\n",
    "¿Reformar la Ley Federal del Trabajo? es una ocurrencia, que no deja de ser una declaración local.\n",
    "Comentó el presidente, ante la propuesta de la fracción del PRI para que el COVID19 sea considerado riesgo de trabajo.\n",
    "Muchas personas viajaron fuera de Colombia para ponerse la vacuna contra Covid19 #uam y el certificado digital de vacunación el 17-02-2023 no los cubre.\n",
    "'''\n",
    "sentencias=sent_tokenize(textos)\n",
    "print(sentencias)\n",
    "print(wordpunct_tokenize(textos))\n",
    "\n",
    "## Imprime sentencias y palabras(tokens)\n",
    "for sentencia in sentencias:\n",
    "    print(sentencia)\n",
    "    print(wordpunct_tokenize(sentencia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAsKVEtqmmL7"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAAnyGlPmoyn",
    "outputId": "a119a537-2f3c-4a12-c2bb-18804a662a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reformar', 'la', 'Ley_Federal_del_Trabajo!']\n",
      "['es', 'una', 'ocurrencia,', 'que', 'no', 'deja', 'de', 'ser', 'una', 'declaración', 'local.']\n",
      "['Comentó', 'el', 'presidente,', 'ante', 'la', 'propuesta', 'de', 'la', 'fracción', 'del', 'PRI', 'para', 'que', 'el', 'COVID19', 'sea', 'considerado', 'riesgo', 'de', 'trabajo.']\n",
      "['Muchas', 'personas', 'viajaron', 'fuera', 'de', 'Colombia', 'para', 'ponerse', 'la', 'vacuna_contra_Covid19', '#uam', 'y', 'el', 'certificado', 'digital', 'de', 'vacunación', 'el', '17-02-2023', 'no', 'los', 'cubre.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenizar multipalabra\n",
    "\n",
    "tk=MWETokenizer([('vacuna','contra','Covid19')])\n",
    "tk.add_mwe(('Ley','Federal',\"del\",'Trabajo!'))\n",
    "\n",
    "for texto in sentencias:\n",
    "  res=tk.tokenize(texto.split())\n",
    "  print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP1ParH471mf"
   },
   "source": [
    "# Limpieza de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ej0dohVu751E",
    "outputId": "e0582f19-bf79-4063-994a-a7502dd0b19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muchas personas viajaron a Colombia para ponerse la vacuna contra #COVID19 y el certificado de vacunación digital \n"
     ]
    }
   ],
   "source": [
    "#Elimina ruido (caracteres especiales)\n",
    "\n",
    "texto=\"Muchas personas viajaron a Colombia para ponerse la vacuna contra #COVID19 y el certificado de vacunación ( digital ) \"\n",
    "lista_ruido=[\"(\",\")\",\"¿\",\"?\",\"/\",\",\"]\n",
    "\n",
    "palabras_sin_ruido=[palabra for palabra in texto.split(' ') if palabra not in lista_ruido]\n",
    "\n",
    "texto_sin_ruido=\" \".join(palabras_sin_ruido)\n",
    "\n",
    "print(texto_sin_ruido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l5zxDb678wD"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VLYXMac7_Ip",
    "outputId": "4842897e-8374-4de2-de66-a6aa8f7dabbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muchas personas viajaron a  para ponerse la vacuna contra  y el certificado de $1 vacunación (digital)\n"
     ]
    }
   ],
   "source": [
    "#Eliminar ruido (elementos no relevantes)\n",
    "\n",
    "texto=\"Muchas personas viajaron a #Colombia para ponerse la vacuna contra #a1a1a1a y el certificado de $1 vacunación (digital)\"\n",
    "\n",
    "texto=re.sub('#[a-zA-Z0-9]+',\"\",texto)\n",
    "\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2rpuOiG8Gmj",
    "outputId": "5e5d1fec-d83c-4517-c430-4658c1db7c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra encontrada: #personas\n",
      "Palabra encontrada: #UAM\n",
      "Palabra encontrada: #Colombia\n",
      "Palabra encontrada: #COVID19\n"
     ]
    }
   ],
   "source": [
    "#Ifdentificar entidades\n",
    "\n",
    "# * -> 0 o más\n",
    "# + -> 1 o más\n",
    "# ? -> 0 o 1\n",
    "# metacaracteres . ^ $ * + ? { } [ ] \\ | ( )\n",
    "# \\w  -> Coincide con cualquier carácter alfanumérico\n",
    "# https://docs.python.org/es/3/howto/regex.html#more-metacharacters\n",
    "\n",
    "texto=\"Muchas #personas #UAM viajaron a #Colombia para ponerse la vacuna contra #COVID19 y el certificado de vacunación (digital)\"\n",
    "\n",
    "patron=re.compile('#\\S+')\n",
    "\n",
    "for palabra in texto.split(' '):\n",
    "  if patron.match(palabra)!=None:\n",
    "      print(\"Palabra encontrada: \"+palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtHR7iXA8LMi",
    "outputId": "aed86ed8-6109-401d-d784-d95e1a333fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muchas personas viajaron a Colombia para ponerse la vacuna contra COVID19 hola y el certificado de 1 vacunación digital\n"
     ]
    }
   ],
   "source": [
    "#Eliminar ruido (elementos no relevantes) usando expresiones regulares\n",
    "\n",
    "texto=\"Muchas personas viajaron a Colombia para ponerse la vacuna contra #COVID19 \\hola y el certificado de $1 (vacunación) [digital]!\"\n",
    "\n",
    "#texto=re.sub(\"[$#¡!¿&@]\",\"\",texto)\n",
    "texto=re.sub(\"[$#\\*¿\\?¡!\\+\\-\\[\\]\\)\\(\\\\\\]\",\"\",texto)\n",
    "\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "np5GH6d88MOJ",
    "outputId": "eac0ed82-e0bc-4e34-e234-3bb8ada0ee19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformar la Ley Federal del Trabajo #LFT es una ocurrencia @presidenciaMEX, deja de ser una declaración local https.//buff.ly/7rYzT5s. Muchas  personas viajaron fuera de Colombia\n"
     ]
    }
   ],
   "source": [
    "#Limpiar texto quitando URL (direcciones de internet)\n",
    "\n",
    "texto = \"Reformar la Ley Federal del Trabajo #LFT es una ocurrencia @presidenciaMEX, deja de ser una declaración local https.//buff.ly/7rYzT5s. Muchas http://ow.ly/J17l30lKhEG personas viajaron fuera de Colombia\"\n",
    "\n",
    "\n",
    "textoSinURL= re.sub(\"(www\\.\\S+)|(http(S)?:\\S+)\", \"\", texto)\n",
    "\n",
    "print(textoSinURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpurd9KY8PHD",
    "outputId": "2388ddc8-a228-4b58-f81a-33208d1bcec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras vacías: \n",
      "{'tendrá', 'tenías', 'entre', 'hube', 'e', 'nuestro', 'estuviéramos', 'habrá', 'serás', 'estábamos', 'tendrás', 'al', 'había', 'habiendo', 'fueses', 'tuyo', 'hubisteis', 'tendréis', 'teníais', 'por', 'los', 'serías', 'fuésemos', 'fuesen', 'las', 'habida', 'estabais', 'hubiésemos', 'fuimos', 'sentido', 'ni', 'estuve', 'estás', 'hayáis', 'donde', 'otros', 'les', 'seamos', 'tuvisteis', 'tened', 'estarán', 'para', 'cual', 'seáis', 'sus', 'hubo', 'qué', 'tendremos', 'habías', 'sentid', 'esa', 'habrán', 'estuviésemos', 'vuestras', 'tuya', 'tenidas', 'uno', 'habríamos', 'os', 'hubimos', 'sean', 'ti', 'muy', 'estoy', 'estuvieses', 'habréis', 'de', 'estuvieran', 'serían', 'tenía', 'está', 'somos', 'estabas', 'esas', 'mí', 'suyas', 'hubiste', 'estuvierais', 'hubieran', 'estaría', 'habrías', 'fueras', 'ante', 'estuviera', 'quien', 'estuvo', 'se', 'estuvieseis', 'habíamos', 'eso', 'sin', 'estaremos', 'fueron', 'erais', 'durante', 'estéis', 'estada', 'hayamos', 'nuestra', 'teníamos', 'hubieron', 'sintiendo', 'estés', 'hubiéramos', 'tendrán', 'mío', 'estarían', 'hubiesen', 'un', 'estados', 'tendría', 'mía', 'tuvieses', 'ya', 'hasta', 'haya', 'tenidos', 'seas', 'otra', 'estuvimos', 'son', 'sentidos', 'era', 'a', 'estar', 'habríais', 'con', 'sí', 'habremos', 'habrían', 'estas', 'vuestros', 'pero', 'fueseis', 'el', 'eras', 'todo', 'estarías', 'tu', 'otras', 'algunas', 'mías', 'soy', 'tuvieron', 'esto', 'hubieras', 'tus', 'tenéis', 'vuestra', 'estuvieron', 'habré', 'eran', 'hubiese', 'seréis', 'tuvieras', 'más', 'le', 'tuviste', 'estado', 'habría', 'tenga', 'hubieses', 'mi', 'has', 'tuyos', 'tuvieseis', 'sea', 'estuvieras', 'tengáis', 'estaréis', 'vuestro', 'porque', 'estaba', 'que', 'sois', 'hubieseis', 'he', 'me', 'estaré', 'también', 'tenían', 'habéis', 'tuviéramos', 'habrás', 'serán', 'él', 'tengo', 'vosotras', 'estemos', 'del', 'estaríamos', 'tuvierais', 'tengamos', 'y', 'seríais', 'suya', 'tuviésemos', 'tenida', 'tuve', 'hemos', 'tendríamos', 'hay', 'ellas', 'seremos', 'siente', 'es', 'hayas', 'este', 'tendrías', 'tiene', 'tendríais', 'esos', 'tendré', 'hubiera', 'fue', 'será', 'tuyas', 'fuisteis', 'habidos', 'nada', 'sería', 'no', 'nos', 'hubierais', 'la', 'estamos', 'una', 'antes', 'sobre', 'algo', 'fueran', 'mucho', 'habidas', 'tuvo', 'tengan', 'suyo', 'teniendo', 'seré', 'ella', 'tuviesen', 'estuviste', 'nuestras', 'fuese', 'éramos', 'míos', 'tengas', 'tenido', 'cuando', 'mis', 'lo', 'tú', 'desde', 'suyos', 'sentidas', 'habían', 'tenemos', 'tienes', 'quienes', 'fuera', 'fuiste', 'su', 'como', 'otro', 'estuviese', 'estad', 'muchos', 'ese', 'habíais', 'tuvieran', 'tuviera', 'nuestros', 'fuerais', 'estuviesen', 'algunos', 'nosotras', 'estén', 'estáis', 'esta', 'estos', 'fui', 'en', 'tuvimos', 'sentida', 'estaban', 'estando', 'ha', 'unos', 'tendrían', 'todos', 'estará', 'habido', 'tanto', 'te', 'han', 'contra', 'hayan', 'fuéramos', 'seríamos', 'yo', 'estaríais', 'poco', 'esté', 'tuviese', 'ellos', 'están', 'o', 'vosotros', 'estuvisteis', 'tienen', 'nosotros', 'estarás', 'estadas', 'eres'}\n",
      "313\n",
      "Texto original:\n",
      "Muchas personas viajaron a Colombia para ponerse la vacuna contra #COVID19 y el certificado de vacunación (digital)\n",
      "17\n",
      "Texto sin palabras vacias:\n",
      "Muchas personas viajaron Colombia ponerse vacuna #COVID19 certificado vacunación (digital)\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Palabras vacías\n",
    "# https://www.ranks.nl/stopwords/spanish\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "palabras_vacias=set(stopwords.words(\"spanish\"))\n",
    "\n",
    "texto=\"Muchas personas viajaron a Colombia para ponerse la vacuna contra #COVID19 y el certificado de vacunación (digital)\"\n",
    "\n",
    "print(\"Palabras vacías: \")\n",
    "print(palabras_vacias)\n",
    "print(len(palabras_vacias))\n",
    "\n",
    "texto_sin_palabras_vacias=\" \".join([palabra for palabra in texto.split(' ') if palabra not in palabras_vacias])\n",
    "\n",
    "print(\"Texto original:\\n\"+texto)\n",
    "print(len(texto.split(' ')))\n",
    "print(\"Texto sin palabras vacias:\\n\"+texto_sin_palabras_vacias)\n",
    "print(len(texto_sin_palabras_vacias.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgyw-8Fk8Vvi"
   },
   "source": [
    "# Estandarización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUCheG8n8ZC-",
    "outputId": "4971a9cd-90c5-4591-9372-aa523fe75f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      "OMG.... que crees!!!... vi en fb un video donde saliste con tu disfraz, LOL.. me hiciste el día!!... TQM amiga\n",
      "Texto sin ruido:\n",
      "OMG que crees vi en fb un video donde saliste con tu disfraz LOL me hiciste el día TQM amiga\n",
      "Texto en minúsculas:\n",
      "omg que crees vi en fb un video donde saliste con tu disfraz lol me hiciste el día tqm amiga\n",
      "Texto final:\n",
      "oh dios mio que crees vi en facebook un video donde saliste con tu disfraz reirse a carcajadas me hiciste el día te quiero mucho amiga\n"
     ]
    }
   ],
   "source": [
    "texto=\"OMG.... que crees!!!... vi en fb un video donde saliste con tu disfraz, LOL.. me hiciste el día!!... TQM amiga\"\n",
    "print(\"Texto original:\\n\"+texto)\n",
    "\n",
    "#Eliminar ruido\n",
    "lista_ruido=[\"!\",\"\\.\",\",\"]\n",
    "for ruido in lista_ruido:\n",
    "  texto=re.sub(ruido,\"\",texto)\n",
    "\n",
    "print(\"Texto sin ruido:\\n\"+texto)\n",
    "\n",
    "#Convertir minúsculas\n",
    "texto=texto.lower()\n",
    "print(\"Texto en minúsculas:\\n\"+texto)\n",
    "\n",
    "#Estandarizar términos\n",
    "term_estandar={'fb':'facebook','tqm':'te quiero mucho','lol':'reirse a carcajadas','omg':'oh dios mio'}\n",
    "palabras=texto.split(' ')\n",
    "texto_aux=[]\n",
    "\n",
    "for palabra in palabras:\n",
    "  if palabra in term_estandar:\n",
    "    palabra=term_estandar[palabra]\n",
    "  texto_aux.append(palabra)\n",
    "\n",
    "texto_nuevo=\" \".join(texto_aux)\n",
    "\n",
    "print(\"Texto final:\\n\"+texto_nuevo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiN11EjqsoS3"
   },
   "source": [
    "# Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTtIyFdLsrsC",
    "outputId": "4464b460-2489-4a43-bee8-3cbc772be14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-03-03 18:19:34.955754: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-03 18:19:37.437921: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-03 18:19:37.438125: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-03 18:19:37.438156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-03 18:19:44.635321: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting es-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from es-core-news-sm==3.4.0) (3.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.10.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.25.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.22.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.5.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.26.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.1.2)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm\n",
    "import es_core_news_sm\n",
    "import spacy\n",
    "pln = es_core_news_sm.load()\n",
    "\n",
    "#Modelos disponibles -> https://spacy.io/models/es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmaWrDe1sxHQ",
    "outputId": "6debdd27-e790-4c6b-c2fd-7dbf1f8a6a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-03-03 18:21:03.594801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-03 18:21:03.594945: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-03 18:21:03.594969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-03 18:21:05.475603: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.4.4) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.8/dist-packages/spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY            VERSION                            \n",
      "es_core_news_sm   >=3.4.0,<3.5.0   \u001b[38;5;2m3.4.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_sm    >=3.4.0,<3.5.0   \u001b[38;5;2m3.4.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiO7-gX0szdF",
    "outputId": "c82934ee-00a2-4d1c-f677-d9522fd6c024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unos --> uno\n",
      "radares --> radar\n",
      "multan --> multar\n",
      "a --> a\n",
      "Mariano --> Mariano\n",
      "Rajoy --> Rajoy\n",
      "por --> por\n",
      "ir --> ir\n",
      "caminando --> caminar\n",
      "demasiados --> demasiado\n",
      "rápidas --> rápida\n"
     ]
    }
   ],
   "source": [
    "texto = pln(\"Unos radares multan a Mariano Rajoy por ir caminando demasiados rápidas\")\n",
    "\n",
    "for palabra in texto:\n",
    "    lema= palabra.lemma_\n",
    "    palabra_orignal=palabra.text\n",
    "    print( palabra_orignal+ ' --> ' + lema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hwy0k_XAs8_d",
    "outputId": "490d6375-5f27-451e-e5c5-41d78b9490a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "eat\n",
      "an\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "#oracion=\"I was very happy\"\n",
    "oracion=\"I ate an apple\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc=nlp(oracion)\n",
    "for palabra in doc:\n",
    "  print(palabra.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LviVbYF7s_4j"
   },
   "source": [
    "# Stemming (Derivación de textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkfNlFmJtK00",
    "outputId": "deba3233-64ce-4a28-f697-b920289d1112"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,wordpunct_tokenize,sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXzn5hs9tTq1",
    "outputId": "97daf9c6-8323-4bd1-d2ff-05a929d5df0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      "Se siente frio.\n",
      "['se', 'sient', 'fri', '.']\n"
     ]
    }
   ],
   "source": [
    "oracion1 = \"Se siente frio.\"\n",
    "#oracion1 = \"Muchas personas viajaron fuera de Colombia para ponerse la vacuna contra Covid19 y el certificado digital de vacunación no los cubre.\"\n",
    "oracion2 = \"Unos radares multan a Mariano Rajoy ya que dió una mordida\"\n",
    "\n",
    "s=word_tokenize(oracion1)\n",
    "\n",
    "esp=SnowballStemmer('spanish')\n",
    "\n",
    "text_r=[esp.stem(token) for token in s]\n",
    "\n",
    "print(\"Texto original:\\n\"+oracion1)\n",
    "print(text_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7be1QCEQNdU"
   },
   "source": [
    "# Etiquetado POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikFma9A4QPei",
    "outputId": "2ad8a430-b943-4057-ed48-b7792b27c7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-03 21:56:23.227051: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Collecting es-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.5.0/es_core_news_sm-3.5.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.22.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.1.3)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm\n",
    "import es_core_news_sm\n",
    "import spacy\n",
    "pln = es_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q917nNxeQSA9",
    "outputId": "9bb33c7b-f1b3-4a51-8d2a-c9a8d5954022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nidia\tNidia\tPROPN\n",
      "Karen\tKaren\tPROPN\n",
      "Serafin\tSerafin\tPROPN\n",
      "Rojas\tRojas\tPROPN\n",
      "come\tcomar\tVERB\n",
      "manzanas\tmanzana\tNOUN\n",
      "rojas\trojo\tADJ\n"
     ]
    }
   ],
   "source": [
    "#texto = pln(\"Juan compró un regalo a María.\")\n",
    "texto = pln(\"Nidia Karen Serafin Rojas come manzanas rojas\")\n",
    "\n",
    "for palabra in texto:\n",
    "    lema= palabra.lemma_\n",
    "    palabra_orignal=palabra.text\n",
    "    pos=palabra.pos_\n",
    "    print( palabra_orignal+ '\\t' + lema + '\\t'+ pos )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewm5xYD4QUDr",
    "outputId": "1da5683a-e26a-4cf7-e20c-8ce3260487a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('red', 'JJ'), ('apple', 'NN'), ('is', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('table', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "texto = \"The red apple is on the table.\"\n",
    "\n",
    "pos_tags=pos_tag(word_tokenize(texto))\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcWJl2xfQUtn"
   },
   "source": [
    "# Árbol de dependencias por palabras y por sintagmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppije_WzQacA",
    "outputId": "617431a6-8847-4963-e23e-8540d36abc41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karen\n",
      "Pedro\n",
      "la escuela\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#texto = pln(\"El niño comió manzanas jugosas y rojas en el parque.\")\n",
    "#texto = pln(\"Nidia Karen Serafin Rojas come manzanas rojas\")\n",
    "texto = pln(\"Karen fue a la tienda y Pedro salio de la escuela\")\n",
    "\n",
    "for sintagma in texto.noun_chunks:\n",
    "    print(sintagma.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xOK9sTgQc4T",
    "outputId": "6f3a3eaa-edfe-4bae-aeb4-36ccda4efda7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karen\tnsubj\n",
      "fue\tROOT\n",
      "a\tadvmod\n",
      "la\tfixed\n",
      "tienda\tpunct\n"
     ]
    }
   ],
   "source": [
    "#Display del arbol de dependencias: #https://demos.explosion.ai/displacy\n",
    "\n",
    "#Etiquetas de relaciones de dependencia: #https://universaldependencies.org/u/dep/\n",
    "\n",
    "#texto = pln(\"María vio a Pedro con el telescopio.\")\n",
    "texto = pln(\"Karen fue a la tienda\")\n",
    "\n",
    "for palabra in texto:\n",
    "    palabra_orignal=palabra.text\n",
    "    relacion_dep=palabra.dep_\n",
    "    print( palabra_orignal+ '\\t' + relacion_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w53gVLSvQgLt"
   },
   "source": [
    "# Reconocimiento de Entidades Nombradas (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYoSyaCMQjk5"
   },
   "outputs": [],
   "source": [
    "#https://demos.explosion.ai/displacy-ent\n",
    "\n",
    "texto = pln(\"Juan vive en México y trabaja en la UAM Azcapotzalco\")\n",
    "\n",
    "for palabra in texto:\n",
    "    palabra_orignal=palabra.text\n",
    "    ner=palabra.ent_type_\n",
    "    pos=palabra.pos_\n",
    "    print( palabra_orignal+ '\\t' + pos + '\\t' + ner)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
